\section{Generativne suparničke mreže}
Nakon nešto dužeg uvoda, napokon možemo pojasniti kako rade generativne suparničke mreže. Osnovna ideja jest natjecanje između dva aktera - jednog ćemo nazvati generator, a drugog diskriminator. Zadaća generatora u ovoj igri je generirati sliku koju diskriminator neće moći razlikovati od slika iz skupa za učenje, a diskriminator samo klasificira slike s ulaza u stvarne ili lažne. Dvije mreže paralelno uče - generator kako generirati što bolje slike, a diskriminator kako uspješno detektirati lažnjake - dok se ne postigne Nashev ekvilibrij.

Prema \citep{nash_eq}, igra se sastoji od skupa igrača, skupa akcija koje igrači mogu napraviti te funkcije dobitka kojima se određena akcija boduje. U našem slučaju, igru bi igrali generator i diskriminator, akcije su kretnje u smjeru suprotnom od gradijenta za određeni iznos, a evaluiramo ih pomoću funkcije gubitka. Nashev ekvilibrij \citep{nash1950equilibrium} situacija je u kojoj bilo koja akcija koju jednostrano poduzme jedan od igrača, donosi do smanjenja profita za tog igrača (kod nas porast funkcije gubitka). Drukčije rečeno, Nashev je ekvilibrij situacija u kojoj je potrebna zajednička akcija igrača da postignu bolji profit. Upravo to je rješenje igre između generatora i diskriminatora - lokalni Nashev ekvilibrij \citep{heusel2017gans} u kojem jednostranom optimizacijom ni diskriminator ni generator ne mogu smanjiti vrijednost svoje funkcije gubitka.

\subsection{Princip rada}
Zašto proces pronalaska optimalnih parametara generatora i diskriminatora promatramo kao igru umjesto kao optimizacijski problem? Odgovor je u tome što svaki model ima svoju funkciju gubitka $J$. Označimo funkciju gubitka generatora s $J_G$, a diskriminatora s $J_D$. Obje funkcije gubitka ovise o parametrima drugog modela, ali na njih nemaju utjecaja, nego mogu modificirati samo svoje parametre, odnosno
$J_G = f(\vec{\theta}_G, \vec{\theta}_D)$, $J_D =  f(\vec{\theta}_G, \vec{\theta}_D)$. Osim toga, rješenje optimizacijskog problema bio bi točka lokalnog minimuma, a rješenje u našem slučaju je točka u kojoj je ostvaren lokalni minimum i funkcije gubitka generatora i funkcije gubitka diskriminatora, odnosno točka koja je lokalno Pareto optimalna.

Generator može biti bilo koja derivabilna funkcija, ali uglavnom su u uporabi duboke neuronske mreže. Ideja je da ta funkcija modelira funkciju gustoće ulaznog skupa tako da kao ulaz primi uzorak iz neke jednostavne distribucije (npr. Gaussove) koji onda mapira na jedan uzorak iz ciljne distribucije. Označimo distribuciju koju predstavlja generator s $p_{model}$. Formalno:
\begin{equation*}
	\vec{z} \sim \mathcal{N}(\vec{\mu}, \Sigma) \rightarrow G(\vec{z}) \sim p_{model}
\end{equation*}
Implementacijski, ulaz $\vec{z}$ ne mora biti jedan vektor ili predan samo kroz ulazni sloj - arhitektura generatora vrlo je fleksibilna i zahtijeva samo da se funkcija na kraju može derivirati.

Diskriminator, nakon što je završeno treniranje, više nije potreban za uzimanje uzoraka.

\subsection{Treniranje}
Predstavimo ukratko kako se odvija treniranje generativnih suparničkih mreža. Za početak, pretpostavimo da smo već definirali funkcije gubitka za generator ($J_G$) i za diskriminator ($J_D$). Prvo uzorkujemo onoliko uzoraka iz odabrane jednostavne distribucije koliko je velika jedna minigrupa te odaberemo jednu minigrupu slika iz skupa za treniranje. Nadalje, generiramo generatorom lažne slike dajući mu kao ulaz minigrupu slučajnih vektora. Koristeći dvije minigrupe slika koje su nam sad na raspolaganju, odredimo gradijente gubitka $J_D$ te ažuriramo vrijednosti parametara diskriminatora $D$. Ovaj postupak ponavljamo $k$ puta. Nakon toga, generiramo još jednu minigrupu lažnih slika. Pomoću njih određujemo gradijent funkcije $J_G$ te ažuriramo parametre generatora $G$. Algoritam \ref{minibatch} pregledno prikazuje objašnjene korake. \\

\begin{algorithm}[H]
\label{minibatch}
\SetAlgoLined
\KwIn{veličina minigrupe $m$, broj koraka ažuriranja diskriminatora $k$, generator $G$, diskriminator $D$}
\For{$i = 0$; $i < k$; $i = i + 1$}{
	$Z$ $\leftarrow$ uzorkuj $m$ uzoraka šuma iz $p_{\vec{z}}$\\
	$M$ $\leftarrow$ odaberi $m$ primjera iz $p_{\mathcal{D}}$\\
	ažuriraj parametre $\vec{\theta}_D$ diskriminatora $D$ gradijentom $\nabla_{\vec{\theta}_D} J_D (M, Z)$
}
$Z$ $\leftarrow$ uzorkuj $m$ uzoraka šuma iz $p_{\vec{z}}$ \\
ažuriraj parametre $\vec{\theta}_G$ generatora $G$ gradijentom $\nabla_{\vec{\theta}_G} J_G (Z)$
\caption{Gradijentni spust s minigrupama za generativne suparničke mreže}
\end{algorithm}

Funkcije gubitka $J_D$ i $J_G$ ovise o formulaciji igre koju igraju generator i diskriminator. Prema izvornom radu \citep{orig_paper}, igra je definirana na sljedeći način:
\begin{equation}
\label{orig_loss}
\min_G \max_D V(G, D) = \mathbb{E}_{\vec{x} \sim p_{\mathcal{D}}}\log D(\vec{x}) + \mathbb{E}_{\vec{z} \sim p_{\vec{z}}} \log (1 - D(G(\vec{z})))
\end{equation}
Uzevši u obzir da je u idealnom slučaju izlaz diskriminatora za stvaran primjer jedan, a za lažan 0, u ovom izrazu diskriminator pokušava maksimizirati očekivanje ispravnih predikcija. S druge strane, generator pokušava minimizirati samo detekciju lažnih slika jer može utjecati samo na drugi član. Izraz \ref{orig_loss}, međutim, nije najpogodniji za optimizaciju gradijentnim spustom. Naime, u početnom stadiju treniranja, diskriminator vrlo lako (s visokom pouzdanošću) razlikuje stvarne od lažnih slika. U tom području, funkcija $f(x) = \log(1 - x)$ sporo se mijenja, što rezultira vrlo slabim ažuriranjem parametara generatora. Zato autori predlažu da se cilj generatora reformulira u maksimizaciju izraza $\mathbb{E}_{\vec{z} \sim p_{\vec{z}}} \log (D(G(\vec{z})))$. Razlog ovome je što parametri u kojem se postiže ekvilibrij ostaju jednaki, ali modificirani izraz pruža jači gradijent generatoru u početnim fazama učenja. S druge strane, izraz za diskriminator se ne mijenja. 

\subsection{Teorijska podloga}
Jedna od karakteristika generativnih suparničkih mreža jest da su asimptotski konzistentan procjenitelj ulazne distribucije - ako pretpostavimo beskonačni kapacitet generatora i diskriminatora te neograničen broj ulaznih primjera, distribucija koju aproksimira generator konvergira prema ulaznoj distribuciji.

Da bismo dokazali ovu tvrdnju, prvo je potrebno pronaći izraz za optimalni diskriminator ako je generator fiksiran. Raspišemo li izraz \ref{orig_loss}, dobivamo:
\begin{align*}
V(G, D) &= \int_{\vec{x}} p_{\mathcal{D}}\log D(\vec{x})dx +\int_{\vec{z}} p_{\vec{z}}(\vec{z})\log(1 - D(G(\vec{z}))dz \\
	 &= \int_{\vec{x}} \left[p_{\mathcal{D}}(\vec{x})\log D(\vec{x}) + p_{model}(\vec{x})\log(1 - D(\vec{x}))\right] dx
\end{align*}
Može se pokazati da je maksimum funkcije $f(x) = a \log x + b \log (1 - x)$ u točki $\frac{a}{a + b}$. Dakle, izraz za optimalni diskriminator jest $D^*(\vec{x}) = \frac{p_{\mathcal{D}}(\vec{x})}{p_{model}(\vec{x}) + p_{\mathcal{D}}(\vec{x})}$. Koristeći dobiveni izraz, funkciju \ref{orig_loss} možemo izraziti tako da ovisi samo o generatoru:
\begin{align}
C(G) &= \max_D V(G, D) = \mathbb{E}_{\vec{x} \sim p_{\mathcal{D}}}\log D(\vec{x}) + \mathbb{E}_{\vec{z} \sim p_{\vec{z}}} \log (1 - D(G(\vec{z}))) \\ 
&= \mathbb{E}_{\vec{x} \sim p_{\mathcal{D}}}\left[\log \frac{p_{\mathcal{D}}(\vec{x})}{p_{model}(\vec{x}) + p_{\mathcal{D}}(\vec{x})})\right] + \mathbb{E}_{\vec{x} \sim p_{model}}\left[\log \frac{p_{model}(\vec{x})}{p_{model}(\vec{x}) + p_{\mathcal{D}}(\vec{x})}\right] \label{training_criterion}
\end{align}
Ostaje pokazati da se minimum izraza \ref{training_criterion} postiže upravo kada je $p_{model} = p_{\mathcal{D}}$. Pretpostavimo li da je to istina, dobivamo $D^*(\vec{x}) = 0.5$ i $C(G) = \log 0.5 + \log 0.5 = -\log 4$. Sada, izraz \ref{training_criterion} možemo napisati kao
\begin{align}
C(G) &= \mathbb{E}_{\vec{x} \sim p_{\mathcal{D}}}\left[\log \frac{p_{\mathcal{D}}(\vec{x})}{p_{model}(\vec{x}) + p_{\mathcal{D}}(\vec{x})}\right] + \mathbb{E}_{\vec{x} \sim p_{model}}\left[\log \frac{p_{model}(\vec{x})}{p_{model}(\vec{x}) + p_{\mathcal{D}}(\vec{x})}\right] - \log 4 + \log 4 \\
&= \mathbb{E}_{\vec{x} \sim p_{\mathcal{D}}}\left[\log \frac{2p_{\mathcal{D}}(\vec{x})}{p_{model}(\vec{x}) + p_{\mathcal{D}}(\vec{x})}\right] + \mathbb{E}_{\vec{x} \sim p_{model}}\left[\log \frac{2p_{model}(\vec{x})}{p_{model}(\vec{x}) + p_{\mathcal{D}}(\vec{x})}\right] - \ln 4 \\
&= -\ln4 + \int_{\vec{x}} p_{\mathcal{D}}(\vec{x}) \ln \frac{2p_{\mathcal{D}}(\vec{x})}{p_{model}(\vec{x}) + p_{\mathcal{D}}(\vec{x})}dx + \int_{\vec{x}} p_{model}(\vec{x}) \ln \frac{2p_{model}(\vec{x})}{p_{model}(\vec{x}) + p_{\mathcal{D}}(\vec{x})}dx \label{KL}
\end{align}
Izraz oblika $\int_{x} p(x) \ln \frac{p(x)}{q(x)}$ nazivamo Kullback-Leiblerova divergencija. To je zapravo mjera različitosti između dvije vjerojatnosne razdiobe, a uobičajeno je označavamo kao $D_{KL}(p \, \Vert \, q)$. Dakle, 
izraz \ref{KL} možemo zapisati kao:
\begin{align}
C(G) = -\ln 4 + D_{KL}(p_{\mathcal{D}} \, \Vert \, \frac{p_{\mathcal{D}} + p_{model}}{2}) + D_{KL}(p_{model} \, \Vert \, \frac{p_{\mathcal{D}} + p_{model}}{2}) \label{final kl}
\end{align}
Kullback-Leiblerova divergencija je uvijek nenegativna vrijednost, a postiže 0 samo kada su dvije razdiobe jednake. Iz ovoga slijedi da se minimum izraza \ref{final kl} postiže kada su razdiobe $p_{\mathcal{D}}$ i $p_{model}$ jednake $\frac{p_{\mathcal{D}} + p_{model}}{2}$, što je jedino moguće ako $p_{\mathcal{D}} = p_{model}$. Ovime smo dokazali asimptotsku konzistentnost generativnih suparničkih mreža.

Međutim, u praksi nemamo na raspolaganju neograničen skup za treniranje i modele beskonačnoga kapaciteta pa ova teorijska garancija optimalnosti ne vrijedi, ali služi kao dobar pokazatelj potencijala ovoga pristupa.