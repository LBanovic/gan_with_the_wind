\section{Dosadašnji napredak}
\label{prijasnji_rad}
U ovom odjeljku, cilj nam je predstaviti radove koji su snažno utjecali na smjer istraživanja generativnih suparničkih mreža. Iako je ovaj radni okvir definiran tek 2014. \citep{orig_paper}, atraktivno je područje u dubokom učenju te je koncept korišten u mnogim izazovima.

Spomenuli smo ranije da se istraživanje generativnih suparničkih mreža ugrubo može podijeliti u dvije grane: unaprijeđivanje kvalitete generiranih slika i poboljšavanje stabilnosti treniranja. Kako su u prethodnim odjeljcima prikazane relevantne tehnike za unaprijeđivanje treniranja, ovdje će fokus biti na napretcima u kvaliteti generiranja.

Jedno od najranijih opažanja jest da generativnim suparničkim mrežama pogoduje znanje o oznakama primjera koje trebaju generirati \citep{mirza2014conditional}. Autori su uveli malu promjenu u formulaciju minimax igre - umjesto izlaza diskriminatora $D(\vec{x})$ i generatora $G(\vec{x})$ promatramo uvjetne izlaze $D(\vec{x}\|\vec{y})$ i $G(\vec{z}\|\vec{y})$, gdje je $\vec{y}$ oznaka primjera u \textit{one-hot} obliku.

Nešto kasnije, predložene su smjernice za dizajn dubokih generativnih suparničkih mreža s konvolucijskim slojevima \citep{radford2015unsupervised} koje su postali polazna točka za istraživanje u idućim godinama. Među smjernicama su, između ostaloga, uporaba normalizacije po grupama, korištenje zglobnice kao aktivacijske funkcije te eliminacija potpuno povezanih skrivenih slojeva u mrežama.

Grupa istraživača iz tvrtke NVIDIA predložili su poboljšanu proceduru učenja generativnih suparničkih mreža koja se temelji na progresivnom učenju različitih rezolucija ulaznih slika, počevši od najmanje \citep{karras2017progressive}. Ovo je varijanta poznate tehnike za treniranje neuronskih mreža pod nazivom nadzirano predtreniranje, prilagođene za generativni problem.

Isti autori su kao nadogradnju prethodnoga modela predložili arhitekturu temeljenu na odvajanju učenja stila (koji je neovisan o rasporedu elemenata na slici) i strukture kojoj ti elementi podliježu \citep{karras2019style}. Ovim su pristupom, osim što su proizveli izvrsne rezultate, omogućili i određen stupanj kontrole nad generiranim slikama, što dotad nije bio slučaj. Stil su modelirali na zanimljiv način. Umjesto da direktno koriste slučajni vektor koji im je na ulazu, prvo ga propuste kroz nekoliko potpuno povezanih slojeva da bi razdvojili učenje ulaznog prostora od učenja viših elemenata slike. Zatim dobiveni rezultat koriste kao reprezentaciju stila koji primjenjuju na svaku rezoluciju u generatoru. Njihov rezultat predstavlja trenutni vrhunac u kvaliteti generiranih slika.

Osim toga, generativne suparničke mreže primijenjene su i na problem prevođenja slike u sliku - poznat primjer je zamjena konja na slikama zebrama i obrnuto \citep{zhu2017unpaired}. Osnovna je ideja vrlo jednostavna. Pretpostavimo da imamo početnu pretpostavku o potrebnom mapiranju i njegovu inverzu. Ukoliko je savršeno mapiranje, primijenimo li prvo njega pa njegov inverz, trebali bismo dobiti početnu sliku. Upravo ovu razliku pokušava minimizirati predloženi model.

Komentirajmo još kratko tehnike stabilizacije treniranja generativnih suparničkih mreža. Grupa autora je u dva rada \citep{kurach2018largescale}, \citep{lucic2017gans} provela brojne eksperimente kojima su ispitali dosad predložene načine učenja, tehnike regularizacije i normalizacije. Ispostavlja se da odabir funkcije gubitka ovisi o skupu podataka na kojemu radimo, ali dodatak spektralne normalizacije i gradijentne kazne poboljšavaju krajnji rezultat. Zato predlažu minimizaciju Kullback-Leiblerove divergencije uz spektralnu normalizaciju kao dobru početnu točku za primjenu generativnih suparničkih mreža na nove skupove podataka. 